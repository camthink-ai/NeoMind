//! Native backend conversation test - tests real model output quality and performance
//!
//! Run with: cargo test -p edge-ai-llm --test native_conversation_test --features native -- --nocapture --test-threads=1

use std::io::Write;
use std::sync::Arc;
use edge_ai_llm::backends::{NativeConfig, NativeRuntime};
use edge_ai_core::{
    llm::backend::{LlmRuntime, LlmInput, GenerationParams},
    Message,
};
use futures::StreamExt;

#[tokio::test]
async fn test_native_basic_conversation() {
    // Initialize logging
    let _ = tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .with_target(false)
        .try_init();

    println!("\n{:=^80}", "");
    println!(" NATIVE BACKEND - BASIC CONVERSATION TEST");
    println!(" Model: qwen3:1.7b (Qwen2.5-1.5B-Instruct from HuggingFace)");
    println!("{:=^80}\n", "");

    // Configure native backend
    let config = NativeConfig::new("qwen3:1.7b")
        .with_device("cpu")
        .with_max_seq_len(2048);

    println!("â³ Loading model... (this may take a while on first run)");
    let runtime = NativeRuntime::new(config).expect("Failed to create runtime");
    let runtime = Arc::new(runtime);

    // Check availability
    println!("ğŸ” Checking model availability...");
    let available = runtime.is_available().await;
    if !available {
        println!("âŒ Model is not available.");
        println!("   This test requires downloading a ~3GB model from HuggingFace.");
        println!("   Model: Qwen/Qwen2.5-1.5B-Instruct");
        println!("   Cache location: ~/.cache/neotalk/models/qwen3-1.7b/");
        println!("\n   To test with Ollama instead (faster, already installed), run:");
        println!("   cargo test -p edge-ai-llm --test conversation_test -- --nocapture");
        println!("\n   Skipping native test for now...");
        return;
    }
    println!("âœ… Model is available!\n");

    // Test cases for basic conversation
    let test_cases = vec![
        ("ç®€å•é—®å€™", "ä½ å¥½ï¼Œè¯·è‡ªæˆ‘ä»‹ç»ä¸€ä¸‹ã€‚"),
        ("æ•°å­¦è®¡ç®—", "25 + 37 ç­‰äºå¤šå°‘ï¼Ÿ"),
        ("å¸¸è¯†é—®ç­”", "ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ"),
        ("è‹±æ–‡å¯¹è¯", "Hello, how are you today?"),
    ];

    let mut total_tests = 0;
    let mut passed_tests = 0;

    for (test_name, user_message) in &test_cases {
        total_tests += 1;
        println!("\n{:-^80}", "");
        println!(" [{}/{}] {}", total_tests, test_cases.len(), test_name);
        println!("{:-^80}", "");
        println!("ç”¨æˆ·: {}\n", user_message);

        let input = LlmInput {
            messages: vec![Message::user(*user_message)],
            params: GenerationParams {
                max_tokens: Some(512),
                temperature: Some(0.7),
                ..Default::default()
            },
            model: Some("qwen3:1.7b".to_string()),
            stream: true,
            tools: None,
        };

        let mut full_response = String::new();
        let mut chunk_count = 0usize;
        let start_time = std::time::Instant::now();

        match runtime.generate_stream(input).await {
            Ok(mut stream) => {
                print!("åŠ©æ‰‹: ");
                std::io::stdout().flush().unwrap();

                loop {
                    match stream.next().await {
                        Some(chunk_result) => match chunk_result {
                            Ok((text, is_thinking)) => {
                                chunk_count += 1;
                                if !is_thinking {
                                    print!("{}", text);
                                    std::io::stdout().flush().unwrap();
                                    full_response.push_str(&text);
                                }
                            }
                            Err(e) => {
                                println!("\nâŒ æµé”™è¯¯: {}", e);
                                break;
                            }
                        }
                        None => break,
                    }
                }

                let elapsed = start_time.elapsed();

                println!("\n\nğŸ“Š ç»Ÿè®¡:");
                println!("  â±ï¸  ç”¨æ—¶: {:.2}s", elapsed.as_secs_f64());
                println!("  ğŸ“¦ æ¥æ”¶å—æ•°: {}", chunk_count);
                println!("  ğŸ“ å›å¤å­—ç¬¦: {}", full_response.chars().count());

                if full_response.chars().count() > 10 {
                    println!("  âœ… æµ‹è¯•é€šè¿‡");
                    passed_tests += 1;
                } else {
                    println!("  âš ï¸  å›å¤è¾ƒçŸ­");
                }
            }
            Err(e) => {
                println!("âŒ è¯·æ±‚å¤±è´¥: {}", e);
            }
        }
    }

    println!("\n{:=^80}", "");
    println!(" æµ‹è¯•æ±‡æ€»");
    println!("{:=^80}", "");
    println!("  æ€»æµ‹è¯•æ•°: {}", total_tests);
    println!("  é€šè¿‡: {} âœ…", passed_tests);
    println!("  æˆåŠŸç‡: {:.1}%", (passed_tests as f64 / total_tests as f64 * 100.0));
    println!("{:=^80}\n", "");

    assert!(passed_tests >= total_tests / 2, "At least half of the tests should pass");
}

#[tokio::test]
async fn test_native_conversation_with_history() {
    let _ = tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .with_target(false)
        .try_init();

    println!("\n{:=^80}", "");
    println!(" NATIVE BACKEND - CONVERSATION WITH HISTORY TEST");
    println!("{:=^80}\n", "");

    let config = NativeConfig::new("qwen3:1.7b")
        .with_device("cpu")
        .with_max_seq_len(2048);

    let runtime = Arc::new(NativeRuntime::new(config).expect("Failed to create runtime"));

    // Check if model is available
    if !runtime.is_available().await {
        println!("âš ï¸  Model not available, skipping test. Run test_basic_conversation first.");
        return;
    }

    // Multi-turn conversation
    let mut messages = vec![
        Message::user("æˆ‘å«å°æ˜ï¼Œæ˜¯ä¸€åç¨‹åºå‘˜ï¼Œå–œæ¬¢æ‰“ç¯®çƒã€‚"),
    ];

    println!("ğŸ“ åˆå§‹ä¿¡æ¯: æˆ‘å«å°æ˜ï¼Œæ˜¯ä¸€åç¨‹åºå‘˜ï¼Œå–œæ¬¢æ‰“ç¯®çƒã€‚\n");

    for (turn, user_msg) in [
        "ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ",
        "æˆ‘çš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿ",
        "æˆ‘å–œæ¬¢ä»€ä¹ˆè¿åŠ¨ï¼Ÿ",
        "è¯·æ€»ç»“ä¸€ä¸‹æˆ‘çš„ä¿¡æ¯",
    ].iter().enumerate() {
        println!("\n{:-^80}", "");
        println!(" ç¬¬ {} è½®å¯¹è¯", turn + 1);
        println!("{:-^80}", "");
        println!("ç”¨æˆ·: {}", user_msg);

        messages.push(Message::user(*user_msg));

        let input = LlmInput {
            messages: messages.clone(),
            params: GenerationParams {
                max_tokens: Some(512),
                temperature: Some(0.7),
                ..Default::default()
            },
            model: Some("qwen3:1.7b".to_string()),
            stream: true,
            tools: None,
        };

        let start_time = std::time::Instant::now();

        match runtime.generate_stream(input).await {
            Ok(mut stream) => {
                let mut response = String::new();
                print!("åŠ©æ‰‹: ");
                std::io::stdout().flush().unwrap();

                loop {
                    match stream.next().await {
                        Some(Ok((text, is_thinking))) => {
                            if !is_thinking {
                                print!("{}", text);
                                std::io::stdout().flush().unwrap();
                                response.push_str(&text);
                            }
                        }
                        Some(Err(e)) => {
                            println!("\nâŒ é”™è¯¯: {}", e);
                            break;
                        }
                        None => break,
                    }
                }

                let elapsed = start_time.elapsed();
                println!("\n  (â±ï¸ {:.2}s, ğŸ“ {} å­—)", elapsed.as_secs_f64(), response.chars().count());

                messages.push(Message::assistant(&response));
            }
            Err(e) => {
                println!("âŒ è¯·æ±‚å¤±è´¥: {}", e);
            }
        }
    }

    println!("\n{:=^80}", "");
    println!(" å¤šè½®å¯¹è¯æµ‹è¯•å®Œæˆ");
    println!("{:=^80}\n", "");
}

#[tokio::test]
async fn test_native_with_tools() {
    let _ = tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .with_target(false)
        .try_init();

    println!("\n{:=^80}", "");
    println!(" NATIVE BACKEND - TOOL CALLING TEST");
    println!("{:=^80}\n", "");

    let config = NativeConfig::new("qwen3:1.7b")
        .with_device("cpu")
        .with_max_seq_len(2048);

    let runtime = Arc::new(NativeRuntime::new(config).expect("Failed to create runtime"));

    // Check if model is available
    if !runtime.is_available().await {
        println!("âš ï¸  Model not available, skipping test.");
        return;
    }

    use edge_ai_core::llm::backend::ToolDefinition;
    use serde_json::json;

    let tools = vec![
        ToolDefinition {
            name: "get_weather".to_string(),
            description: "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯".to_string(),
            parameters: json!({
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°"
                    }
                },
                "required": ["city"]
            }),
        },
        ToolDefinition {
            name: "get_time".to_string(),
            description: "è·å–å½“å‰æ—¶é—´".to_string(),
            parameters: json!({
                "type": "object",
                "properties": {},
                "required": []
            }),
        },
    ];

    let user_message = "è¯·é—®åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿç°åœ¨å‡ ç‚¹äº†ï¼Ÿ";
    println!("ç”¨æˆ·: {}\n", user_message);

    let input = LlmInput {
        messages: vec![
            Message::user(user_message),
        ],
        params: GenerationParams {
            max_tokens: Some(512),
            temperature: Some(0.7),
            ..Default::default()
        },
        model: Some("qwen3:1.7b".to_string()),
        stream: true,
        tools: Some(tools),
    };

    let start_time = std::time::Instant::now();

    match runtime.generate_stream(input).await {
        Ok(mut stream) => {
            let mut response = String::new();
            print!("åŠ©æ‰‹: ");
            std::io::stdout().flush().unwrap();

            loop {
                match stream.next().await {
                    Some(Ok((text, is_thinking))) => {
                        if !is_thinking {
                            print!("{}", text);
                            std::io::stdout().flush().unwrap();
                            response.push_str(&text);
                        }
                    }
                    Some(Err(e)) => {
                        println!("\nâŒ é”™è¯¯: {}", e);
                        break;
                    }
                    None => break,
                }
            }

            let elapsed = start_time.elapsed();
            println!("\n\nğŸ“Š ç»Ÿè®¡:");
            println!("  â±ï¸  ç”¨æ—¶: {:.2}s", elapsed.as_secs_f64());
            println!("  ğŸ“ å›å¤å­—ç¬¦: {}", response.chars().count());

            // Check if tool calling format is present
            if response.contains("get_weather") || response.contains("get_time") {
                println!("  âœ… æ¨¡å‹å°è¯•è°ƒç”¨å·¥å…·");
            } else {
                println!("  âš ï¸  æ¨¡å‹æ²¡æœ‰ä½¿ç”¨å·¥å…·è°ƒç”¨æ ¼å¼");
            }
        }
        Err(e) => {
            println!("âŒ è¯·æ±‚å¤±è´¥: {}", e);
        }
    }

    println!("\n{:=^80}\n", "");
}
